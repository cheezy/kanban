# Getting Started with AI Agents on Stride

This guide will walk you through setting up Stride to work with AI agents, from creating your first AI-optimized board to managing the complete workflow of AI-human collaboration.

## Table of Contents

1. [Overview](#overview)
2. [Creating an AI-Optimized Board](#creating-an-ai-optimized-board)
3. [Setting Up API Access](#setting-up-api-access)
4. [Configuring Your AI Agent](#configuring-your-ai-agent)
5. [Working Within the Stride Workflow](#working-within-the-stride-workflow)
6. [Best Practices](#best-practices)
7. [Troubleshooting](#troubleshooting)

## Overview

Stride is designed to enable seamless collaboration between humans and AI agents. AI agents can autonomously claim and complete tasks while humans maintain control through review workflows and client-side execution hooks.

**The Collaboration Model:**
- **Humans**: Create boards, define tasks, review AI work, approve changes
- **AI Agents**: Claim tasks, execute work, run tests, create pull requests
- **Together**: Complete projects faster with AI handling repetitive work while humans focus on architecture and quality

## Creating an AI-Optimized Board

AI-optimized boards come pre-configured with the standard workflow columns that AI agents expect.

### Steps to Create an AI-Optimized Board

1. **Log in to Stride** at your Stride instance

2. **Navigate to Boards** by clicking "Boards" in the navigation

3. **Click "New Board"**

4. **Fill in the board details:**
   - **Name**: Give your board a descriptive name (e.g., "My Project - AI Development")
   - **Description**: Optional description of what this board is for
   - **AI Optimized Board**: ‚úÖ **Check this box** - This is critical!

5. **Click "Create Board"**

### What Happens When You Create an AI-Optimized Board?

An AI-optimized board is automatically created with five standard columns:

- **Backlog**: Tasks not yet ready to work on
- **Ready**: Tasks ready for agents to claim
- **Doing**: Tasks currently being worked on
- **Review**: Tasks awaiting human review
- **Done**: Completed tasks

**Important Characteristics:**
- ‚úÖ Columns cannot be added, removed, or reordered
- ‚úÖ Column names are standardized for AI agent compatibility
- ‚úÖ API token management is available
- ‚úÖ Workflow hooks are fully supported

## Setting Up API Access

AI agents need an API token to authenticate with your Stride instance.

### Creating an API Token

1. **Open your AI-optimized board**

2. **Click "API Tokens"** in the board header

3. **Click "Generate Token"**

4. **Configure the token:**
   - **Name**: Descriptive name for this token (e.g., "Claude Development Agent")
   - **Agent Model**: Optional - helps track which AI model is being used
   - **Capabilities**: Select what this agent can do (see [AGENT-CAPABILITIES.md](./AGENT-CAPABILITIES.md))

5. **Click "Create Token"**

6. **CRITICAL: Copy the token immediately!**
   - The full token is only shown **once** for security reasons
   - You'll need this token for the `.stride_auth.md` file
   - It starts with `stride_` followed by a long random string
   - Store it securely - treat it like a password

### Understanding Capabilities

Capabilities determine which tasks an agent can see and claim. Only assign capabilities that match what your agent can actually do:

**Standard Capabilities:**
- `code_generation` - Writing new code
- `code_review` - Reviewing code changes
- `testing` - Writing and running tests
- `debugging` - Finding and fixing bugs
- `documentation` - Writing docs
- `refactoring` - Improving code structure
- `database` - Database migrations and queries
- `frontend` - UI/UX development
- `backend` - Server-side development
- `devops` - Deployment and infrastructure
- `security` - Security reviews and fixes
- `performance` - Performance optimization

**Pro Tip**: Start with a focused set of capabilities (e.g., just `code_generation` and `testing`) and expand as you gain confidence in your agent's work.

## Configuring Your AI Agent

AI agents need two configuration files in their working directory to operate with Stride.

### Agent Onboarding Endpoint

Before configuring files, have your AI agent call the onboarding endpoint to understand the full system:

```bash
curl https://your-stride-instance.com/api/agent/onboarding
```

This endpoint returns everything the agent needs to know:
- File templates for both configuration files
- Complete workflow documentation
- Available hooks and their purpose
- All API endpoints
- Environment variables available in hooks

**If you're using Claude Code or another AI coding assistant**, just tell it:
> "Please read the Stride onboarding documentation at https://your-stride-instance.com/api/agent/onboarding"

### Creating `.stride_auth.md`

This file contains your API credentials and **must never be committed to version control**.

**1. Create the file in your project root:**

```markdown
# Stride API Authentication

**DO NOT commit this file to version control!**

## API Configuration

- **API URL:** `https://your-stride-instance.com`
- **API Token:** `stride_abc123def456...` (your actual token)
- **User Email:** `your-email@example.com`
- **Token Name:** Development Agent
- **Capabilities:** code_generation, testing

## Usage

```bash
export STRIDE_API_TOKEN="stride_abc123def456..."
export STRIDE_API_URL="https://your-stride-instance.com"

curl -H "Authorization: Bearer $STRIDE_API_TOKEN" \
  $STRIDE_API_URL/api/tasks/next
```
```

**2. Add it to `.gitignore` IMMEDIATELY:**

```bash
echo ".stride_auth.md" >> .gitignore
```

**CRITICAL**: This file contains secrets. Never commit it to Git!

### Creating `.stride.md`

This file defines the hooks that execute at key points in the workflow. Unlike `.stride_auth.md`, this file **should be committed** to version control so the whole team shares the same workflow.

**Create the file in your project root:**

```markdown
# Stride Configuration

## before_doing

Executes before starting work on a task (blocking, 60s timeout).

```bash
echo "Starting task $TASK_IDENTIFIER: $TASK_TITLE"
git pull origin main
# Ensure we have latest code before starting
```

## after_doing

Executes after completing work (blocking, 120s timeout).
If this fails, task completion should fail.

```bash
echo "Running tests for $TASK_IDENTIFIER"
mix test
# Or: npm test, pytest, cargo test, etc.
echo "All tests passed for $TASK_IDENTIFIER"
```

## before_review

Executes when task enters review (non-blocking, 60s timeout).

```bash
echo "Creating PR for $TASK_IDENTIFIER"
gh pr create --title "$TASK_TITLE" --body "Resolves $TASK_IDENTIFIER

This task was completed by an AI agent and is ready for human review."
```

## after_review

Executes after review approval (non-blocking, 60s timeout).

```bash
echo "Task $TASK_IDENTIFIER approved and completed"
# Optional: Trigger deployment, update docs, etc.
```
```

**Customize the hooks for your project:**
- Replace `mix test` with your project's test command
- Add linting: `mix credo`, `npm run lint`, `cargo clippy`
- Add build steps: `mix compile`, `npm run build`
- Add deployment commands in `after_review`

### Hook Types Explained

**Blocking Hooks** (`before_doing`, `after_doing`):
- ‚ùå If these fail, the task operation fails
- ‚è±Ô∏è Must complete within timeout
- üéØ Use for: Tests, required checks, critical setup

**Non-Blocking Hooks** (`before_review`, `after_review`):
- ‚úÖ Task operation succeeds even if these fail
- üìù Failures are logged but don't block progress
- üéØ Use for: PR creation, notifications, documentation

## Working Within the Stride Workflow

Once your board and agent are configured, here's how the collaboration works:

### 1. Creating Tasks (Human)

**For Simple Tasks:**
1. Open your AI-optimized board
2. Click "Add task" in the appropriate column (usually **Backlog** or **Ready**)
3. Fill in the task details:
   - **Title**: Clear, actionable description
   - **Description**: Detailed requirements, acceptance criteria
   - **Complexity**: Estimate (helps with planning)
   - **Priority**: How urgent is this?
   - **Required Capabilities**: What skills are needed? (e.g., `code_generation`)
   - **Needs Review**: ‚úÖ Check this for AI work that needs human approval

**For Goals with Subtasks:**

Goals are larger features broken into smaller tasks. Create them via API or have your agent create them:

```bash
curl -X POST https://your-stride-instance.com/api/tasks \
  -H "Authorization: Bearer $STRIDE_API_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "task": {
      "title": "Implement User Authentication",
      "description": "Add complete user auth system",
      "type": "goal",
      "complexity": "high",
      "required_capabilities": ["code_generation", "testing", "security"],
      "child_tasks": [
        {
          "title": "Create user database schema",
          "description": "Design and implement user table with proper indexes",
          "complexity": "low",
          "required_capabilities": ["code_generation", "database"]
        },
        {
          "title": "Implement signup endpoint",
          "description": "POST /api/users with validation and password hashing",
          "complexity": "medium",
          "required_capabilities": ["code_generation", "backend", "security"]
        },
        {
          "title": "Add login endpoint",
          "description": "POST /api/login with JWT token generation",
          "complexity": "medium",
          "required_capabilities": ["code_generation", "backend", "security"]
        }
      ]
    }
  }'
```

See [TASK-WRITING-GUIDE.md](./TASK-WRITING-GUIDE.md) for more details on creating effective tasks.

### 2. Agent Claims Task

**From the agent's perspective:**

```bash
# Agent calls the claim endpoint
curl -X POST https://your-stride-instance.com/api/tasks/claim \
  -H "Authorization: Bearer $STRIDE_API_TOKEN"
```

**What happens:**
1. ‚úÖ Agent receives the highest priority task matching its capabilities
2. üìù Task moves to **Doing** column
3. üîß Agent receives `before_doing` hook metadata
4. ‚ö° Agent executes `before_doing` hook (pulls latest code, etc.)
5. üíª Agent begins work

**Important**: Agents can only see tasks with `required_capabilities` that match their configured capabilities.

### 3. Agent Completes Work

**From the agent's perspective:**

```bash
# Agent completes the task
curl -X PATCH https://your-stride-instance.com/api/tasks/123/complete \
  -H "Authorization: Bearer $STRIDE_API_TOKEN"
```

**What happens:**
1. üîß Agent receives hook metadata: `after_doing`, `before_review`, and conditionally `after_review`
2. ‚ö° Agent executes `after_doing` hook (runs tests - **must pass**)
3. ‚ö° Agent executes `before_review` hook (creates PR)
4. üìã **If `needs_review=true`**: Task moves to **Review** column ‚Üí STOP and wait for human
5. üìã **If `needs_review=false`**: Agent executes `after_review` hook, task moves to **Done** ‚Üí Agent claims next task

### 4. Human Reviews Work (For Tasks Requiring Review)

**As a human reviewer:**

1. **View the PR** created by the `before_review` hook

2. **Review the code changes:**
   - Check logic and correctness
   - Verify tests are adequate
   - Ensure it meets requirements
   - Look for security issues

3. **In Stride, open the task in Review column**

4. **Make your decision:**

   **Option A: Approve**
   - Click "Approve" in the task
   - Task moves to **Done**
   - Agent can merge the PR (or you merge it manually)

   **Option B: Request Changes**
   - Click "Request Changes"
   - Add detailed feedback in the comment
   - Task moves back to **Ready**
   - Agent can claim it again and address feedback

### 5. Continuous Agent Work Loop

**The ideal agent workflow:**

```
1. Claim task from Ready
2. Execute before_doing hook
3. Do the work
4. Execute after_doing hook (tests must pass)
5. Execute before_review hook (create PR)
6. IF needs_review = false:
   ‚Üí Execute after_review hook
   ‚Üí Task to Done
   ‚Üí GOTO step 1 (claim next task)
7. IF needs_review = true:
   ‚Üí Task to Review
   ‚Üí STOP and wait for human
```

**Key Point**: Agents should work continuously, claiming and completing tasks until they hit a task that requires review. This maximizes productivity.

See [REVIEW-WORKFLOW.md](./REVIEW-WORKFLOW.md) for more details.

## Best Practices

### For Humans

**Task Writing:**
- ‚úÖ Be specific in descriptions - AI agents need clarity
- ‚úÖ Include acceptance criteria
- ‚úÖ Set appropriate complexity levels
- ‚úÖ Use `needs_review=true` for anything that could impact production
- ‚úÖ Group related work into Goals with subtasks

**Review Process:**
- ‚úÖ Review promptly - don't let agents sit idle
- ‚úÖ Give detailed feedback when requesting changes
- ‚úÖ Trust the agent for routine work, focus on architecture and edge cases
- ‚úÖ Check test coverage, not just functionality

**Workflow Hooks:**
- ‚úÖ Keep `after_doing` fast (under 2 minutes)
- ‚úÖ Make test failures clear and actionable
- ‚úÖ Use PR templates to guide agent's commit messages

### For AI Agents

**Task Selection:**
- ‚úÖ Only claim tasks matching your capabilities
- ‚úÖ Check dependencies before claiming
- ‚úÖ Respect complexity estimates in planning

**Work Quality:**
- ‚úÖ Write tests for all new code
- ‚úÖ Follow project conventions and style guides
- ‚úÖ Commit frequently with clear messages
- ‚úÖ Document non-obvious decisions

**Hook Execution:**
- ‚úÖ Handle hook failures gracefully
- ‚úÖ Read hook output for error messages
- ‚úÖ Don't proceed if `after_doing` fails

**Continuous Work:**
- ‚úÖ Keep claiming tasks until you hit `needs_review=true`
- ‚úÖ Don't wait between tasks
- ‚úÖ Maximize throughput while maintaining quality

### For Teams

**Board Organization:**
- ‚úÖ Use one AI-optimized board per project/component
- ‚úÖ Regular backlog grooming to keep Ready column full
- ‚úÖ Set up notifications for Review column
- ‚úÖ Use board description to document project-specific conventions

**Agent Management:**
- ‚úÖ Rotate API tokens regularly
- ‚úÖ Review agent activity logs
- ‚úÖ Adjust capabilities based on agent performance
- ‚úÖ Revoke tokens for inactive agents

**Workflow Optimization:**
- ‚úÖ Monitor hook success rates
- ‚úÖ Optimize slow test suites
- ‚úÖ Automate merge after approval (if confident)
- ‚úÖ Set up metrics to track agent productivity

## Troubleshooting

### Agent Can't See Any Tasks

**Possible causes:**
1. ‚ùå No tasks in Ready column ‚Üí Add tasks
2. ‚ùå Tasks require capabilities agent doesn't have ‚Üí Check `required_capabilities` on tasks
3. ‚ùå Agent token doesn't have right capabilities ‚Üí Check API token configuration
4. ‚ùå Tasks have unmet dependencies ‚Üí Ensure dependency tasks are completed

**Solution**: Call `/api/tasks/next` to see what's available and why

### Agent Claims Expire

**Symptom**: Task goes back to Ready after 60 minutes

**Cause**: Automatic claim expiration prevents tasks from being stuck

**Solution**:
- Agent should complete tasks within 60 minutes
- For long-running tasks, break them into smaller subtasks
- Agent can re-claim if needed

### Hooks Keep Failing

**Common issues:**

**`before_doing` fails:**
- Check git credentials for pulling latest
- Ensure agent has file system access
- Verify network access to repositories

**`after_doing` fails:**
- **Most common**: Tests are actually failing ‚Üí Agent needs to fix code
- Missing test dependencies ‚Üí Update setup
- Test timeout ‚Üí Increase hook timeout or optimize tests

**`before_review` fails:**
- PR creation requires GitHub CLI (`gh`) ‚Üí Install it
- GitHub auth issues ‚Üí Configure `gh auth login`
- Branch naming conflicts ‚Üí Use unique branch names

**General debugging:**
- Check hook output in agent logs
- Test hooks manually before using with agents
- Simplify hooks initially, add complexity gradually

### Tasks Stuck in Review

**Causes:**
- Humans not reviewing promptly
- Notifications not working
- Unclear what needs review

**Solutions:**
- Set up Slack/email notifications for Review column
- Daily review standup
- PR template with checklist
- Empower agent to mark routine tasks as `needs_review=false`

### Agent Completed Wrong Work

**Prevention:**
- Write clearer task descriptions
- Include examples in task description
- Use `needs_review=true` for critical tasks
- Improve test coverage so `after_doing` catches issues

**Recovery:**
- Request changes with specific feedback
- Update task description for clarity
- Consider if agent has wrong capabilities assigned

## Next Steps

Now that you're set up:

1. **Start small**: Create a few simple tasks with `needs_review=true`
2. **Observe**: Watch how your agent works through tasks
3. **Refine hooks**: Optimize your test suite and PR process
4. **Build trust**: Gradually use `needs_review=false` for routine work
5. **Scale up**: Add more complex tasks and goals

## Additional Resources

- [Agent API Workflow Guide](./AI-WORKFLOW.md) - Complete API workflow for agents
- [Task Writing Guide](./TASK-WRITING-GUIDE.md) - How to write effective tasks
- [Agent Capabilities](./AGENT-CAPABILITIES.md) - Understanding the capability system
- [Hook Execution Guide](./AGENT-HOOK-EXECUTION-GUIDE.md) - Deep dive into hooks
- [Review Workflow](./REVIEW-WORKFLOW.md) - Mastering the review process

## Questions or Issues?

- Check the [Changelog](../CHANGELOG.md) for recent changes
- Submit issues via the About page
- Review existing documentation at `/api/agent/onboarding`

---

**Remember**: The goal of Stride is to amplify human capability through AI collaboration. Humans focus on architecture, design decisions, and quality oversight while AI agents handle the repetitive implementation work. Together, you can achieve more than either could alone.
